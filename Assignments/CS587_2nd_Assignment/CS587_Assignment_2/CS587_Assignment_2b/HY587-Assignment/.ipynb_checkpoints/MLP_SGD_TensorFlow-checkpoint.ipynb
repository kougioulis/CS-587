{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Build, Train & Test a Multilayer Neural Networks using TensorFlow\n",
    "\n",
    "\n",
    "### Goals: \n",
    "- Intro: build and train a feed forward neural network using the `TensorFlow` framework.\n",
    "- The SGD method will be used for training to apply automatic differentiation based on TensorFlow.\n",
    "- Tune the hyperparameters and modify the structure of your NN to achieve the highest accuracy.\n",
    "- Use Tensorboard to visualize the graph and results.\n",
    "\n",
    "### Dataset:\n",
    "- Digits: 10 class handwritten digits\n",
    "- It will automatically be downloaded once you run the provided code using the scikit-learn library.\n",
    "- Check for info in the following websites:\n",
    "- http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html\n",
    "- http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# display figures in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- normalization of your input data\n",
    "- train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=numpy.nan)\n",
    "\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "X_train = data[0:1500,:]\n",
    "y_train = target[0:1500]\n",
    "\n",
    "X_test = data[1500:,:]\n",
    "y_test = target[1500:]\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Check that the train and test targets/labels are balanced within each set\n",
    "plt.hist(y_train)  # plt.hist passes it's arguments to np.histogram\n",
    "plt.title(\"Histogram of train labels/targets\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(y_test)  # plt.hist passes it's arguments to np.histogram\n",
    "plt.title(\"Histogram of test labels/targets\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the one of the transformed sample (after feature standardization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 150\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(X_train[sample_index].reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"transformed sample\\n(standardization)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler objects makes it possible to recover the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(scaler.inverse_transform(X_train[sample_index]).reshape(8, 8),\n",
    "           cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.title(\"original sample\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow is a symbolic graph computation engine, that allows automatic differentiation of each node\n",
    "- https://www.tensorflow.org \n",
    "- https://www.tensorflow.org/tutorials/mnist/tf/\n",
    "\n",
    "TensorFlow builds where nodes may be:\n",
    "- **constant:** constants tensors, such as a learning rate\n",
    "- **Variables:** any tensor, such as parameters of the models\n",
    "- **Placeholders:** placeholders for inputs and outputs of your models\n",
    "- many other types of nodes (functions, loss, ...)\n",
    "\n",
    "The graph is symbolic, no computation is performed until a `Session` is defined and the command `run` or `eval` is invoked. TensorFlow may run this computation on (multiple) CPUs or GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(3)\n",
    "b = tf.constant(2)\n",
    "c = tf.Variable(0)\n",
    "c = a + b\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float32\", name=\"input\")\n",
    "Y = X + tf.constant(3.0)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(Y, feed_dict={X:2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: batches in inputs**\n",
    "- the first dimension of the input is usually kept for the batch dimension. A typical way to define an input placeholder with a 1D tensor of 128 dimensions, is:\n",
    "```\n",
    "X = tf.placeholder(\"float32\", shape=[None, 128])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Build a model using TensorFlow\n",
    "\n",
    "- Using TensorFlow, build a simple model (one hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "def accuracy(y_pred, y=y_test):\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD YOUR MODEL, LOSS, PREDICT & TRAIN OPERATORS & INIT ALL VARIABLES IN THE GRAPH\n",
    "\n",
    "\n",
    "# hyperparams\n",
    "batch_size = 32\n",
    "hid_size = 15\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "input_size = X_train.shape[1]\n",
    "output_size = 10\n",
    "\n",
    "# input and output\n",
    "X = tf.placeholder(\"float32\", shape=[None, input_size])\n",
    "y = tf.placeholder(\"int32\", shape=[None])\n",
    "\n",
    "#todo: build the model and weights\n",
    "W_h = []\n",
    "b_h = []\n",
    "W_o = []\n",
    "b_o = []\n",
    "\n",
    "#define/set your activation function and the output of your net\n",
    "h = []\n",
    "out_act = []\n",
    "\n",
    "#todo: build the loss using softmax cross entropy error function, \n",
    "# mock loss and b, you need to change , use sparse_softmax_cross_entropy_with_logits\n",
    "b = init_weights([output_size])\n",
    "loss = b\n",
    "\n",
    "#todo: define/set your optimizer (Stochastic Gradient Descent) using the given learning rate value\n",
    "#and the train operator, use tf.train.GradientDescentOptimizer(learning_rate) for the optimizer\n",
    "optimizer = []\n",
    "train_op = []\n",
    "\n",
    "#todo: build predict node using the softmax error function\n",
    "predict = X\n",
    "\n",
    "#todo: define the summaries!\n",
    "\n",
    "# Initialization of all variables in the graph\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train your model using SGD algorithm and check the generalization on the test set of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init your session, run training\n",
    "#Render your graph and monitor your training procedure using TensorBoard\n",
    "\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)\n",
    "    \n",
    "    losses = []\n",
    "    for e in range(num_epochs):\n",
    "        for i in range(X_train.shape[0] // batch_size):\n",
    "            # Build batches of batch_size            \n",
    "            idx, idxn = i * batch_size, min(X_train.shape[0]-1, (i+1) * batch_size)\n",
    "            batch_xs, batch_ys = X_train[idx: idxn], y_train[idx: idxn]            \n",
    "            \n",
    "            # Run train operator for the current batch and save/monitor loss.\n",
    "            # todo: adjust the next command (s) to also add the summaries. \n",
    "            # You can add as many lines as you need\n",
    "            _, l = sess.run([train_op, loss], feed_dict={X: batch_xs, y: batch_ys})\n",
    "            losses.append(l)\n",
    "        \n",
    "        #to-do: For each epoch, run accuracy on train and test.\n",
    "        predicts_test = []\n",
    "        predicts_train = []\n",
    "        \n",
    "        #to-do: call the accuracy function to compute accuracy scores and print them\n",
    "        acc_train = 0\n",
    "        acc_test = 0\n",
    "        print(\"epoch: %d train accuracy: %0.3f test accuracy: %0.3f\"\n",
    "              % (e, acc_train, acc_test))\n",
    "    \n",
    "    # For monitoring purposes\n",
    "    file_writer = tf.summary.FileWriter('./tensorflow_summaries', sess.graph)\n",
    "    \n",
    "plt.plot(losses);\n",
    "\n",
    "\n",
    "#IN CASE YOU MISSED IT \n",
    "#to-do Do not forget to USE TENSORBOARD for graph rendereing + monitoring your training process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3) In order to maximize the accuracy on the given dataset try different settings for your model\n",
    "\n",
    "Play around with the structure of your NN model and fine-tune its hyperparameters.\n",
    "\n",
    "- A. Experiment with di fferent hyperparameters (learning rate = 0.001,..,0.1, batch size = 8,..,128, size of hidden layers = 5,..,25, number of epochs).\n",
    "- B. Try di fferent activation functions (e.g., ReLU, TanH).\n",
    "- C. Try to add more hidden layers and increase their size.\n",
    "- D. Add L2 regularization (e.g., with regularization strength 10^(-4))\n",
    "\n",
    "### Bonus: A + 15% will be distributed to the top-performing models based on the accuracy on the test set (e.g if there are K submissions with equal top performance, each one will get a bonus 15%/K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to-do- MAXimize the accuracy on the given dataset try different settings for your model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
